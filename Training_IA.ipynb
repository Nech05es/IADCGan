{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1ihospcwI/8B9RY7MRGNP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXSQyT79nVoz",
        "outputId": "380e680a-3e19-4c01-b8a8-798f4c909f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt, warnings\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import load_img, array_to_img\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ksbiO05EoAE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ipath = '/content/drive/My Drive/DSAF/'\n",
        "Mpath = '/content/drive/My Drive/models/'"
      ],
      "metadata": {
        "id": "3P3T_JUopfmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.load(Ipath + 'Training_data.npy')\n",
        "# reshape imagenes\n",
        "train_imgs = dataset.reshape(dataset.shape[0], 64, 64, 3).astype('float32')\n",
        "# normalize the images\n",
        "train_imgs = (train_imgs - 127.5) / 127.5"
      ],
      "metadata": {
        "id": "85_tMGrPpip-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGAN(keras.Model):\n",
        "    def __init__(self, generator, discriminator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
        "        self.d_loss_metric = keras.metrics.Mean(name='d_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.g_loss_metric, self.d_loss_metric]\n",
        "\n",
        "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
        "        super(DCGAN, self).compile()\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # get batch size from the data\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        # generate random noise\n",
        "        random_noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # train the discriminator with real (1) and fake (0) images\n",
        "        with tf.GradientTape() as tape:\n",
        "            # compute loss on real images\n",
        "            pred_real = self.discriminator(real_images, training=True)\n",
        "            # generate real image labels\n",
        "            real_labels = tf.ones((batch_size, 1))\n",
        "            # label smoothing\n",
        "            real_labels += 0.05 * tf.random.uniform(tf.shape(real_labels))\n",
        "            d_loss_real = self.loss_fn(real_labels, pred_real)\n",
        "\n",
        "            # compute loss on fake images\n",
        "            fake_images = self.generator(random_noise)\n",
        "            pred_fake = self.discriminator(fake_images, training=True)\n",
        "            # generate fake labels\n",
        "            fake_labels = tf.zeros((batch_size, 1))\n",
        "            d_loss_fake = self.loss_fn(fake_labels, pred_fake)\n",
        "\n",
        "            # total discriminator loss\n",
        "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "\n",
        "        # compute discriminator gradients\n",
        "        gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        # update the gradients\n",
        "        self.d_optimizer.apply_gradients(zip(gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "\n",
        "        # train the generator model\n",
        "        labels = tf.ones((batch_size, 1))\n",
        "        # generator want discriminator to think that fake images are real\n",
        "        with tf.GradientTape() as tape:\n",
        "            # generate fake images from generator\n",
        "            fake_images = self.generator(random_noise, training=True)\n",
        "            # classify images as real or fake\n",
        "            pred_fake = self.discriminator(fake_images, training=True)\n",
        "            # compute loss\n",
        "            g_loss = self.loss_fn(labels, pred_fake)\n",
        "\n",
        "        # compute gradients\n",
        "        gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        # update the gradients\n",
        "        self.g_optimizer.apply_gradients(zip(gradients, self.generator.trainable_variables))\n",
        "\n",
        "        # update states for both models\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "\n",
        "        return {'d_loss': self.d_loss_metric.result(), 'g_loss': self.g_loss_metric.result()}"
      ],
      "metadata": {
        "id": "Tu23CTf6m6Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_imgs=25, latent_dim=100):\n",
        "        self.num_imgs = num_imgs\n",
        "        self.latent_dim = latent_dim\n",
        "        self.noise = tf.random.normal([num_imgs, latent_dim])\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        g_img = self.model.generator(self.noise, training=False)\n",
        "        g_img = (g_img * 127.5) + 127.5\n",
        "        g_img = g_img.numpy()\n",
        "\n",
        "        fig, axes = plt.subplots(5, 5, figsize=(8, 8))\n",
        "        for i in range(self.num_imgs):\n",
        "            ax = axes[i // 5, i % 5]\n",
        "            img = array_to_img(g_img[i])\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "        plt.savefig(Ipath + 'genImg/epoch_{:03d}.png'.format(epoch))\n",
        "        plt.show()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.model.generator.save(Mpath + 'generatorDgan.h5')\n",
        "        self.model.discriminator.save(Mpath + 'discriminatorDgan.h5')\n",
        "        pass"
      ],
      "metadata": {
        "id": "TIXGWuDznARd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = tf.keras.models.load_model(Mpath + 'generatorDgan.h5')\n",
        "discriminator = tf.keras.models.load_model(Mpath + 'discriminatorDgan.h5')"
      ],
      "metadata": {
        "id": "2fxoBX7imuJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dcgan = DCGAN(generator=generator, discriminator=discriminator, latent_dim=100)"
      ],
      "metadata": {
        "id": "8ZFCzGNamxqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D_LR = 0.0001\n",
        "G_LR = 0.0003\n",
        "dcgan.compile(g_optimizer=Adam(learning_rate=G_LR, beta_1=0.5), d_optimizer=Adam(learning_rate=D_LR, beta_1=0.5), loss_fn=BinaryCrossentropy())"
      ],
      "metadata": {
        "id": "nNk_q_kwm1cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 2\n",
        "dcgan.fit(train_imgs, epochs=N_EPOCHS, callbacks=[DCGANMonitor()])"
      ],
      "metadata": {
        "id": "8zelzABvpHgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "noise = tf.random.normal([1, 100])\n",
        "fig = plt.figure(figsize=(3, 3))\n",
        "# generate the image from noise\n",
        "g_img = dcgan.generator(noise)\n",
        "# denormalize the image\n",
        "g_img = (g_img * 127.5) + 127.5\n",
        "g_img.numpy()\n",
        "img = array_to_img(g_img[0])\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.savefig(Ipath + 'genImg/epoch_{:03d}.png'.format(random.randint(0,1000)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l80QuZ2Dm21Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}